{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcvPurFisLmWKHYQAkbIUm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bongjoonsiong/Generative-AI/blob/main/Real_Time_Financial_News_RAG_Chatbot_with_Gemini_and_Qdrant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building Real-Time Financial News RAG Chatbot with Gemini and Qdrant\n",
        "What if you had at your disposal a real-time financial news chatbot that could provide you with all the news related to finance and economics? Does that sound interesting? Retrieval Augmented Generation (RAG) has now made this possible. We can leverage large language models and vector databases to get our queries answered.\n",
        "Let’s create a real-time financial news RAG chatbot and see if it accurately answers questions using available data. It can be real-time by feeding a vector database with the latest news."
      ],
      "metadata": {
        "id": "5QjvfFvOqgH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install the required dependencies"
      ],
      "metadata": {
        "id": "Pwe9X5k3qBq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q llama-index 'google-generativeai>=0.3.0' qdrant_client llama-index-embeddings-fastembed fastembed llama-index-llms-gemini\n"
      ],
      "metadata": {
        "id": "oYbM6DalSIKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing the Node\n",
        "As the dataset is a CSV file, let’s load the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Slm4wfXPSR8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "docs = SimpleDirectoryReader(\"Dataset\").load_data()"
      ],
      "metadata": {
        "id": "LuCVvDafSEtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the documents are ready; now we will split the sentences into defined chunk sizes."
      ],
      "metadata": {
        "id": "gqQUKxkUSZqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser.text import SentenceSplitter\n",
        "\n",
        "# Initialize the SentenceSplitter with a specific chunk size\n",
        "text_parser = SentenceSplitter(chunk_size=1024)\n",
        "\n",
        "text_chunks = [] # This will hold all the chunks of text from all documents\n",
        "doc_idxs = [] # This will keep track of the document each chunk came from\n",
        "\n",
        "for doc_idx, doc in enumerate(docs):\n",
        "\n",
        " # Split the current document's text into chunks\n",
        " cur_text_chunks = text_parser.split_text(doc.text)\n",
        "\n",
        " # Extend the list of all text chunks with the chunks from the current document\n",
        " text_chunks.extend(cur_text_chunks)\n",
        "\n",
        " # Extend the document index list with the index of the current document, repeated for each chunk\n",
        " doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
        ""
      ],
      "metadata": {
        "id": "Ji1vyLdnSpcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we will create a text node object and assign the metadata to it. We will store all the nodes in one node list.\n",
        "\n"
      ],
      "metadata": {
        "id": "ONc1ToEZSvxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.schema import TextNode\n",
        "\n",
        "nodes = [] # This will hold all TextNode objects created from the text chunks\n",
        "\n",
        "# Iterate over each text chunk and its index\n",
        "\n",
        "for idx, text_chunk in enumerate(text_chunks):\n",
        "\n",
        " # Create a TextNode object with the current text chunk\n",
        " node = TextNode(text=text_chunk)\n",
        "\n",
        " # Retrieve the source document using the current index mapped through doc_idxs\n",
        " src_doc = docs[doc_idxs[idx]]\n",
        "\n",
        " # Assign the source document's metadata to the node's metadata attribute\n",
        " node.metadata = src_doc.metadata\n",
        "\n",
        " # Append the newly created node to the list of nodes\n",
        " nodes.append(node)\n",
        ""
      ],
      "metadata": {
        "id": "fkfwqf1lSthY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Qdrant Vector Store\n",
        "\n",
        "To store the nodes, we need a vector store. Here, we have chosen Qdrant as our vector store. Qdrant is a high-performance vector database with all the specific features that a vector store should have. It is fast and accurate by utilizing the HNSW algorithm for approximate nearest neighbor search. Qdrant supports additional payload and filters based on payload values by providing an easy-to-use API. Additionally, it supports docker installation, is equipped with in-memory storage of vectors, is cloud-native, and scales horizontally. Developed in the Rust language, Qdrant implements dynamic query planning and payload data indexing.\n",
        "\n",
        "First, we’ll create a collection in the vector store index."
      ],
      "metadata": {
        "id": "tKrxF30tUlvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, StorageContext\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.core import Settings\n",
        "\n",
        "import qdrant_client\n",
        "\n",
        "# Create a local Qdrant vector store\n",
        "client = qdrant_client.QdrantClient(path=\"financialnews\")\n",
        "vector_store = QdrantVectorStore(client=client, collection_name=\"collection\")"
      ],
      "metadata": {
        "id": "PzKzUA-KUyec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini Embeddings and Text Model\n",
        "The vector store and nodes are ready, but the vector store is not going to directly accept the nodes. They require embeddings, and for embeddings, we are using the Gemini embedding model here. We’ll be leveraging the Gemini LLM, which is a very capable family of multimodal models. Built on the transformer architecture and trained on TPUs, the Gemini model excels in summarization, reading comprehension tasks with per-task fine-tuning, multilinguality, long context, coding, complex reasoning, mathematics, and of course, multimodality.\n",
        "\n",
        "We’ll initiate the Google API key, which you can obtain from Google AI Studio."
      ],
      "metadata": {
        "id": "TtFqhbttFqqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env GOOGLE_API_KEY = \"your-api-key\"\n",
        "import os\n",
        "GOOGLE_API_KEY = \"your-api-key\" # add your GOOGLE API key here\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "kHfto0fUFxMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, using the API key, we will generate the embeddings using the FastEmbed embedding model and the Gemini LLM in Llamaindex’s Settings.\n",
        "\n"
      ],
      "metadata": {
        "id": "huq__X3NFyZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "for node in nodes:\n",
        " node_embedding = embed_model.get_text_embedding(\n",
        " node.get_content(metadata_mode=\"all\")\n",
        " )\n",
        " node.embedding = node_embedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "Settings.embed_model = embed_model\n",
        "Settings.llm = Gemini(model=\"models/gemini-pro\")\n",
        "Settings.transformations = [SentenceSplitter(chunk_size=1024)]\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex(\n",
        " nodes=nodes,\n",
        " storage_context=storage_context,\n",
        "transformations=Settings.transformations,\n",
        ")"
      ],
      "metadata": {
        "id": "ytneoUt_F30K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vector store is saved in the storage context, and the index has been initiated with it and the nodes."
      ],
      "metadata": {
        "id": "2b0cTgOeug4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HyDE Query Transformation\n",
        "Now, we’ll initiate the vector query engine with a response synthesizer and vector retriever. Vector Retriever is initiated with the vector index retriever in which the index was included. Response Synthesizer generates a response from an LLM, using a user query and a given set of text chunks. The output of a response synthesizer is a Response Object."
      ],
      "metadata": {
        "id": "-TQa9AJbup8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=2)\n",
        "response_synthesizer = get_response_synthesizer()\n",
        "vector_query_engine = RetrieverQueryEngine(\n",
        " retriever=vector_retriever,\n",
        " response_synthesizer=response_synthesizer,\n",
        ")"
      ],
      "metadata": {
        "id": "01m6AxkEuuq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will employ the HyDE query transformer for advanced retrieval. HyDE (Hypothetical Document Embeddings) facilitates zero-shot prompt-based instruction-following in a large language model. It generates a hypothetical document encapsulating relevant text patterns, converts them into embedding vectors, and averages them to create a single embedding. This procedure identifies the corresponding actual embedding through vector similarity in the document embedding space, thereby eliminating the need for a retrieval step involving querying an input and obtaining a document from a large database.\n",
        "\n",
        "The HyDE query transformation assists in delivering responses directly and concisely."
      ],
      "metadata": {
        "id": "BDkqnhHXuz0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
        "from llama_index.core.query_engine import TransformQueryEngine\n",
        "hyde = HyDEQueryTransform(include_original=True)\n",
        "hyde_query_engine = TransformQueryEngine(vector_query_engine, hyde)"
      ],
      "metadata": {
        "id": "A14376h1umb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leveraging Gradio UI for Chatbot Implementation\n",
        "For deploying a chatbot, we will use Gradio."
      ],
      "metadata": {
        "id": "o7DkbsDCug_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def queries(query_str):\n",
        " response = hyde_query_engine.query(query_str)\n",
        " return str(response)\n",
        "import gradio as gr\n",
        "import os\n",
        "gr.close_all()\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        " gr.Markdown(\n",
        " \"\"\"\n",
        " # Welcome to Gemini-Powered Stock Predictor RAG Chatbot!\n",
        " \"\"\")\n",
        " chatbot = gr.Chatbot()\n",
        " msg = gr.Textbox()\n",
        " clear = gr.ClearButton([msg, chatbot])\n",
        " def respond(message, chat_history):\n",
        " bot_message = queries(message)\n",
        " chat_history.append((message, bot_message))\n",
        " return \"\", chat_history\n",
        " msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "RAayRM9Ju-nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the chatbot and enquiry about Finanical News!"
      ],
      "metadata": {
        "id": "Mv3sGoukvAnQ"
      }
    }
  ]
}